\section{Results and Discussion}
\label{sec:results}
\subsubsection{Experiment 1.}
The results of this can be seen in Table~\ref{tab:baselines}.
The best results we achieved, using the graph neural net with $\textsc{NIter}=2$ and including all AST, \textsc{Variable}, and \textsc{Token} edges, can be seen in Table~\ref{tab:results:ast}.
We also include an analysis of the false positive and negative rates on the test set in Table~\ref{tab:test-fps}.
These results show that the graph neural net framework, along with the assumptions we made about the graph structure, work well to model the problem: our solution roughly halves the error rate from random guessing, and significantly outperforms all other baselines.
Further, the false positive and negative analysis in Table~\ref{tab:test-fps} shows only two significant outliers, the number of false predictions on the \texttt{string} type and the number of missed predictions of the \texttt{number} type.
Given that some of the most common operators on numbers and strings are syntactically identical in TypeScript ($+$ adds numbers and concatenates strings), it is unsurprising that numbers would get interpreted as strings, especially considering that the MLE guess when the two are syntactically indistinguishable would be \texttt{string}; this intuition has been confirmed by checking that the majority of missed \texttt{number} predictions are predicted as \texttt{string}s.

\subsubsection{Experiment 2.} The results of these studies can be seen in Tables~\ref{tab:results:ast}, \ref{tab:results:variable}, and \ref{tab:results:token}.
These experiments confirm our assumptions, that each of the types of edges we selected help in solving the type inference problem.
We can additionally see the relative benefit of each of the edge types: the syntactic locality induced by the \textsc{Token} edges seems to matter more than the AST edges, which in turn matter more than the \textsc{Variable} edges.
While it is somewhat surprising that the \textsc{Variable} edges are the least important, since type inference is based in part on variable usage, this can be explained in part by the relatively small number of message passing iterations: there is no immediate information gained from a linked variable, only from that linked variable's neighbors, so more iterations would likely be needed to see as much benefit from the \textsc{Variable} edges.

\begin{table}
 \centering
 {\renewcommand{\arraystretch}{1.3}% for the vertical padding
 \begin{tabular}{c|lcc}
 	 ~ & \textbf{Model} & \textbf{P@1} & \textbf{P@5} \\
 	\hline
 	Our model & GNN & 0.67 & 0.74 \\
 	\hline
 	 ~ & Naive & 0.72 & 0.82 \\
 	 ~	 & (Majority vote) & ~ & ~ \\
	\cline{2-4}
	 Baselines & Intermediate & 0.72 & 0.82 \\
 	 ~	 & (Dependency counts) & ~ & ~ \\
 	\cline{2-4}
	 ~	 & Aggressive & 0.72 & 0.82 \\
 	 ~	 & (Bi-direc RNN) & ~ & ~ \\
 \end{tabular}
}
 \caption{Results of model comparison against various baselines, ordered by their model complexity. These are results on the test-set, on the top 20 most frequent labels in the train-set.}
 \label{tab:results:baselines}
\end{table}

\begin{table}
  \centering
  \begin{subfigure}{\linewidth}
    \centering
    \begin{tabular}{c|ccccc}
      & \textbf{Top 1} & \textbf{Top 2} & \textbf{Top 3} & \textbf{Top 4} & \textbf{Top 5} \\
      \hline
      Train & 0.37 & 0.67 & 0.74 & 0.79 & 0.83 \\
      Test & 0.31 & 0.72 & 0.82 & 0.85 & 0.87
    \end{tabular}
    \caption{Baseline: MLE}\label{tab:results:mle}
  \end{subfigure}

  \medskip

  \begin{subfigure}{\linewidth}
    \centering
    \begin{tabular}{c|ccccc}
      & \textbf{Top 1} & \textbf{Top 2} & \textbf{Top 3} & \textbf{Top 4} & \textbf{Top 5} \\
      \hline
      Train & 0.69 & 0.85 & 0.91 & 0.94 & 0.96 \\
      Test & 0.66 & 0.83 & 0.91 & 0.94 & 0.96
    \end{tabular}
    \caption{Our best results (\textsc{NIter}=2)}\label{tab:results:pure}
  \end{subfigure}

  \medskip

  \begin{subfigure}{\linewidth}
    \centering
    \begin{tabular}{c|ccccc}
      & \textbf{Top 1} & \textbf{Top 2} & \textbf{Top 3} & \textbf{Top 4} & \textbf{Top 5} \\
      \hline
      Train & 0.58 & 0.77 & 0.85 & 0.89 & 0.92 \\
      Test & 0.55 & 0.76 & 0.86 & 0.91 & 0.93
    \end{tabular}
    \caption{Ablation: no AST Edges}\label{tab:results:ast}
  \end{subfigure}

  \medskip

  \begin{subfigure}{\linewidth}
    \centering
    \begin{tabular}{c|ccccc}
      & \textbf{Top 1} & \textbf{Top 2} & \textbf{Top 3} & \textbf{Top 4} & \textbf{Top 5} \\
      \hline
      Train & 0.64 & 0.82 & 0.88 & 0.92 & 0.94 \\
      Test & 0.61 & 0.80 & 0.86 & 0.89 & 0.92
    \end{tabular}
    \caption{Ablation: no Variable Connections}\label{tab:results:variable}
  \end{subfigure}

  \medskip

  \begin{subfigure}{\linewidth}
    \centering
    \begin{tabular}{c|ccccc}
      & \textbf{Top 1} & \textbf{Top 2} & \textbf{Top 3} & \textbf{Top 4} & \textbf{Top 5} \\
      \hline
      Train & 0.50 & 0.73 & 0.83 & 0.88 & 0.92 \\
      Test & 0.43 & 0.74 & 0.97 & 0.92 & 0.94
      \end{tabular}
      \caption{Ablation: No Token Edges}\label{tab:results:token}
  \end{subfigure}
  \caption{Top 5 train and test accuracies}
  \label{tab:results}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{lrrr}
    \textbf{Label} & \textbf{\#Correct} & \textbf{\#Missed} & \textbf{\#Falsely Predicted} \\
    \hline
    \texttt{string} & 6374 & 1021 & 4216 \\
    \texttt{number} & 5501 & 4006 & 881 \\
    \texttt{boolean} & 1610 & 850 & 1089 \\
    \texttt{any[]} & 367 & 240 & 226 \\
    \texttt{string[]} & 130 & 321 & 141 \\
    \texttt{() => void} & 606 & 279 & 190 \\
    \texttt{A}  & 0 & 0 & 301 \\
    \texttt{undefined} & 37 & 58 & 179 \\
    \texttt{() => string} & 91 & 112 & 111
  \end{tabular}
  \caption{Top 10 test labels (23551 total predictions)}\label{tab:test-fps}
\end{table}

\subsubsection{Experiment 3.}
\textbf{Bayesian Optimization.}
The metric we chose to optimize over was the train loss after 3 epochs.
Selecting the best performing number of iterations in this metric gave us confidence that the network had actually made progress in learning significant features of the dataset, while also being sure not to overfit to our validation set.
The results of this experiment can be seen in Figure~\ref{fig:gp-diagram}, which shows the confidence bounds on the number of iterations to use in the range $[0, 10]$, which gave us a satisfactory coverage of the average path length seen in Figure~\ref{fig:dataset-graph-stats}, while still being computationally tractable to train.
After 5 interactions with the Gaussian process, we were confident that $n=1$ iteration gave the best training loss after 3 epochs, although we used $n=2$ in our experiments, since it seemed to do marginally better on the validation set.

We were slightly surprised by the relatively low number of iterations found as ideal by the Bayesian Optimization.
Ultimately, we believe that this is probably an issue with the amount of time that we trained for: since a message passing graph net unfurls to look similar to a RNN, it encounters the same vanishing gradient problem, magnified by the large fanout of each node (Figure~\ref{fig:dataset-graph-stats} shows an average node degree above 5).
Were we to train for significantly longer, we may be able to deal with the smaller gradients and the more complex loss landscape of a larger network, but unfortunately training the larger \textsc{NIter} networks for longer on our machines was computationally infeasible -- we leave this deeper parameter exploration to future work.
\\~\\
\textbf{Iteration Ensemble.}
Ultimately, this did not work as well as we hoped, and our best results were achieved by just using a single number of iterations learned through the Bayesian Optimization approach.
The linear combination weight vector changed only marginally from its initial value, and the prediction quality was no better than that of using a fixed number of iterations.
We believe this happened for one primary reason: one or two iterations were good enough, as shown by the Bayesian Optimization approach.
Any iterations beyond that did not refine the search, and since we did not penalize their existence through any regularization (only the wrongness of their predictions), they stayed around as unnecessary artifacts that did not get pruned.
\loadFig{gpDiagram}
