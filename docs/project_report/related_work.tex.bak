\section{Related Work}
\label{sec:related_work}
Recent work have focused on detecting and classifying vulnerabilities through traditional program analysis techniques \cite{sym},\cite{taint},\cite{song}, while others focus on efficient and meaningful representations of programs for modeling and prediction in software engineering applications \cite{summarize}\cite{codecomplete},\cite{am}. However, few have worked at the intersection of the two. Our work is closest to unpublished (available on arXiv) work by \cite{draper}. They also use static analysers for ground truth in C, C++ programs. However, they learn representations of programns by training a CNN on a bag of lexicalized tokens, and then use a Random Forest classifier, which requires large amounts of data. Our work instead focuses on the design of highly interpretable features which capture variable interactions. Our design works in a low-data setting which DSLs like Solidity present.

Work regarding program representations close to ours are \cite{kdd}, \cite{pldi}. \cite{am} introduce features derived from \use-\define paths of programs extracted from ASTs. They consider the two terminal nodes of such paths, and use the control and data-context of those nodes. They do not capture long-range dependencies among variables, spanning multiple functions and classes. We introduce a more general representation for long range dependencies, comprising \textit{usages} and \textit{defines} of variables. Moreover, real-world programs are noisier than restircted student submissions. \cite{pldi} represents programs as AST paths where they consider all valid paths between variables appearing in a program, and train these \textit{bag of paths} using an RNN, again require large amounts of data. 
