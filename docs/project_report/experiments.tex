\section{Experiments}
\label{sec:experiments}
We investigate the following questions in this work, and set up experiments to answer them.

\begin{itemize}[noitemsep,topsep=0pt]
	\item \textbf{E1.} Do GNNs capture well the inductive bias in representing prediction tasks with a graph-like structure?
	\item \textbf{E2.} How does the nature of structural information passed along the edges in a GNN affect its predictability?
	\item \textbf{E3.} How can the number of iterations, a critical hyperparameter which controls message passing in graph networks, be determined automatically, as against the current way of setting them apriori?
\end{itemize}

\subsubsection{Experiment 1. Representation using GNN.}
We investigate whether the inductive bias captured in the structure of graph neural networks help in prediction tasks on graph-like modalities. To determine this, we model the task of predicting types of variables using GNN. To ascertain whether the architecture naturally captures this prediction task, we set up a number of baselines to compare against. We investigated the following models -
\begin{itemize}[noitemsep,topsep=0pt]
	\item \textbf{Our model. GNN.} We model the type inference problem using GNNs. Each node represents a node in the program's AST. We infer the types on those nodes which pertain to a variable. This is the model we compare all other baselines against.
	\item \textbf{Baseline 1. Naive.} We design a naive baseline to compare performance. In this model, we perform a majority vote on the label distribution in our train-set, and use the most frequently occurring label as the type predicted for any unseen variable. This provides a maximum likelihood estimate.
	\item \textbf{Baseline 2. Intermediate.} Increasing the complexity of our baseline models, we evaluate a naive approximation of a graph net. In this model, we construct a logistic regression model over a feature vector involving \textit{counts} of various dependency and node information of a program. Specifically, for each variable $v$ in each program in the corpus, we enumerate the following counts
	\begin{itemize}[noitemsep,topsep=0pt]
		\item For each variable $v_c (v_c\not=v)$ in the program which has an edge to $v$, the path formed by AST nodes present along that edge is counted. This forms a \textit{count}-hot vector of all such unique paths (string of AST nodes) that connect a variable $v$.
		\item A \textit{count}-hot encoding of the AST node present in the line of code containing $v$, and the line of code containing $v_c$.
	\end{itemize}
	We enumerated 6183 unique paths and 89 unique AST nodes in the training set. This resulted in a 524507$\times$(6183+89) data matrix, which was used to learn a regularized Logistic Regression model. This baseline captures a \textit{bag-of-dependencies}, without the structural/directional information provided by GNNs. Being a linear model, we achieve interpretability on what features get selected.
	\item \textbf{Baseline 3. Aggressive.} We use the bi-directional neural network architecture designed by Allamanis et al \cite{hellendoorn2018deep}. Their architecture does not utilize any of the graphical-properties of a program, and instead rely on signals obtained just by the token information. Their approach is typical of sequence prediction tasks in NLP. Figure \ref{fig:birnn} describes their architecture.
\end{itemize}
\loadFig{baseline}
\textit{Performance metric.} For each of these models, we compute two measures of accuracies: \textsc{P@1}, \textsc{P@5}. A \textsc{P@}$k$ accuracy corresponds to the model's top $k$ predictions containing the true label. This is a standard accuracy measure used in classification tasks.

\subsubsection{Experiment 2. Edge information - Ablation study.}
To determine whether our graph structure (nodes and edges) was chosen correctly, we performed a set of ablation studies.
Specifically, we ran experiments using the optimally chosen hyperparameters where we entirely deleted each class of edges (AST edges, Token edges, and Variable edges) from the graph, and compared performance to the original.

\subsubsection{Experiment 3. Number of iterations.}
A shortcoming in the graph net framework is the issue of determining how many iterations to run.
In theory, because of the universal approximation properties of neural nets, the optimal number of iterations is any number larger than the diameter of the graph, so that each node can make a fully informed decision knowing the entire graph topology.
However, this is not realistic: as shown in Figure~\ref{fig:dataset-graph-stats}, the maximum diameter of any of the graphs is above 200, meaning that we would have to run the graph neural net for an intractable number of iterations to be able to get the ``full'' result.
Instead, we approximate the result by running fewer iterations, theoretically losing out on certain classes of decisions (although further enforcing our inductive bias, since we believe that local nodes should be the primary ones that matter).
It is not immediately clear how we should choose the \textsc{NIter} hyperparameter though: since graph nets give no formal guarantees that the results monotonically improve with the number of iterations, we have to select the number of iterations through some hyperparameter search.
We ran two experiments to try to select the ideal settings: 
\begin{itemize}
	\item \textbf{\emph{Bayesian Optimization} based approach.} 
	We were inspired by approaches to hyperparameter search problems, for cases where running experiments can be prohibitively expensive. In particular we were inspired by \cite{snoek2012practical}, which suggests using a Bayesian Optimization based approach with an underlying Gaussian Process to pick hyperparameters from, using the expected improvement metric (expectation of the decrease in the desired metric over the current best known hyperparameter) to explore the hyperparameter space.
	\item \textbf{\emph{Iteration Ensemble} approach.} Our second attempt at hyperparameter search followed a more optimization based approach.
	Rather than trying to pick one specific ideal number of iterations, we ran an ensemble over several choices of iteration counts, using a learned weighting to linearly combine all of their predictions in the last step to produce the final result.
	Importantly, we ran this iteration ensemble on \emph{one single graph net}; that is, we combined each of the intermediate predictions of running a single graph neural net.
	We considered this for two reasons: primarily, it was more computationally efficient (running large ensembles of graph nets was not possible on the machines we had access to), but it also enforced one of the inductive biases we hoped to see in a solution: intermediate steps of the graph net message passing algorithm should roughly correspond to finer approximations of the posterior prediction, meaning all steps should be valid (if unrefined) predictions. We also hoped to see situations where some weights on the upper or lower end would drop away, signifying that we should shift the number of iterations that comprise the ensemble.
\end{itemize}