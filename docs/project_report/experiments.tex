\section{Experiments}
\label{sec:experiments}
We investigate the following questions in this work, and set up experiments to answer them.

\begin{itemize}[noitemsep,topsep=0pt]
	\item \textbf{E1.} Do GNNs capture well the inductive bias in representing prediction tasks with a graph-like structure?
	\item \textbf{E2.} How does the nature of structural information passed along the edges in a GNN affect its predictability?
	\item \textbf{E3.} How can the number of iterations, a critical hyperparameter which controls message passing in graph networks, be determined automatically, as against the current way of setting them apriori?
\end{itemize}

\subsubsection{Experiment 1. Representation using GNN.}
We investigate whether the inductive bias captured in the structure of graph neural networks help in prediction tasks on graph-like modalities. To determine this, we model the task of predicting types of variables using GNN. To ascertain whether the architecture naturally captures this prediction task, we set up a number of baselines to compare against. We investigated the following models -
\begin{itemize}[noitemsep,topsep=0pt]
	\item \textbf{Our model. GNN.} We model the type inference problem using GNNs. Each node represents a node in the program's AST. We infer the types on those nodes which pertain to a variable. This is the model we compare all other baselines against.
	\item \textbf{Baseline 1. Naive.} We design a naive baseline to compare performance. In this model, we perform a majority vote on the label distribution in our train-set, and use the most frequently occurring label as the type predicted for any unseen variable. This provides a maximum likelihood estimate.
	\item \textbf{Baseline 2. Intermediate.} Increasing the complexity of our baseline models, we evaluate a naive approximation of a graph net. In this model, we construct a logistic regression model over a feature vector involving \textit{counts} of various dependency and node information of a program. Specifically, for each program in the corpus, we enumerate the following counts
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Edges between every \textsc{VariableUse} and \textsc{VariableDefine}, as described in Section \ref{sec:approach}. This forms a \textit{count}-hot vector of all such unique edge-dependencies that appear in the training corpus.
		\item A  \textit{count}-hot encoding of the AST node types that appear in a program.
	\end{itemize}
	We enumerated XX different edge types and YY unique AST nodes in the training set. This resulted in a AAxBB feature matrix, which was used to learn a Logistic Regression model. This baseline captures a \textit{bag-of-dependencies}, without the structural/directional information provided by GNNs.
	\item \textbf{Baseline 3. Aggressive.} We use the bi-directional neural network architecture designed by Allamanis et al \cite{hellendoorn2018deep}. Their architecture does not utilize any of the graphical-properties of a program, and instead rely on signals obtained just by the token information. Their approach is typical of sequence prediction tasks in NLP. Figure \ref{fig:birnn} describes their architecture.
\end{itemize}
\loadFig{baseline}
\subsubsection{Experiment 2. Edge information - Ablation study.}
To determine whether our graph structure (nodes and edges) was chosen correctly, we performed a set of ablation studies.
Specifically, we ran experiments using the optimally chosen hyperparameters where we entirely deleted each class of edges (AST edges, Token edges, and Variable edges) from the graph, and compared performance to the original.
The results of these studies can be seen in Tables~\ref{tab:results:ast}, \ref{tab:results:variable}, and \ref{tab:results:token}.
These experiments confirm our assumptions, that each of the types of edges we selected help in solving the type inference problem.
We can additionally see the relative benefit of each of the edge types: the syntactic locality induced by the \textsc{Token} edges seems to matter more than the AST edges, which in turn matter more than the \textsc{Variable} edges.
While it is somewhat surprising that the \textsc{Variable} edges are the least important, since type inference is based in part on variable usage, this can be explained in part by the relatively small number of message passing iterations: there is no immediate information gained from a linked variable, only from that linked variable's neighbors, so more iterations would likely be needed to see as much benefit from the \textsc{Variable} edges.

\subsubsection{Experiment 3. Number of iterations.}
A shortcoming in the graph net framework is the issue of determining how many iterations to run.
In theory, because of the universal approximation properties of neural nets, the optimal number of iterations is any number larger than the diameter of the graph, so that each node can make a fully informed decision knowing the entire graph topology.
However, this is not realistic: as shown in Figure~\ref{fig:dataset-graph-stats}, the maximum diameter of any of the graphs is above 200, meaning that we would have to run the graph neural net for an intractable number of iterations to be able to get the ``full'' result.
Instead, we approximate the result by running fewer iterations, theoretically losing out on certain classes of decisions (although further enforcing our inductive bias, since we believe that local nodes should be the primary ones that matter).
It is not immediately clear how we should choose the \textsc{NIter} hyperparameter though: since graph nets give no formal guarantees that the results monotonically improve with the number of iterations, we have to select the number of iterations through some hyperparameter search.
We ran two experiments to try to select the ideal settings: 
\begin{itemize}
	\item \textbf{\emph{Bayesian Optimization} based approach.} 
	We were inspired by approaches to hyperparameter search problems, for cases where running experiments can be prohibitively expensive. In particular we were inspired by \cite{snoek2012practical}, which suggests using a Bayesian Optimization based approach with an underlying Gaussian Process to pick hyperparameters from, using the expected improvement metric (expectation of the decrease in the desired metric over the current best known hyperparameter) to explore the hyperparameter space.
	\item \textbf{\emph{Iteration Ensemble} approach.} Our second attempt at hyperparameter search followed a more optimization based approach.
	Rather than trying to pick one specific ideal number of iterations, we ran an ensemble over several choices of iteration counts, using a learned weighting to linearly combine all of their predictions in the last step to produce the final result.
	Importantly, we ran this iteration ensemble on \emph{one single graph net}; that is, we combined each of the intermediate predictions of running a single graph neural net.
	We considered this for two reasons: primarily, it was more computationally efficient (running large ensembles of graph nets was not possible on the machines we had access to), but it also enforced one of the inductive biases we hoped to see in a solution: intermediate steps of the graph net message passing algorithm should roughly correspond to finer approximations of the posterior prediction, meaning all steps should be valid (if unrefined) predictions. We also hoped to see situations where some weights on the upper or lower end would drop away, signifying that we should shift the number of iterations that comprise the ensemble.
\end{itemize}